# process_datasets_recursively.py

import os
import shutil
import json
import numpy as np
import pandas as pd
import argparse
from pathlib import Path

# ==============================================================================
# --- å¸®åŠ©å‡½æ•° (æ¥è‡ª process.py) ---
# ==============================================================================

def calc_terminated_flag(actions, threshold=5.0):
    """
    ä»åå‘å‰è®¡ç®—ç»ˆæ­¢æ ‡å¿—ã€‚
    æœ€åä¸€å¸§flagä¸º1ï¼Œéå†è¿‡ç¨‹ä¸­å¦‚æœå½“å‰actionä¸åä¸€å¸§actionçš„diffå¤§äºé˜ˆå€¼ï¼Œ
    åˆ™å½“å‰å¸§åŠä¹‹å‰æ‰€æœ‰å¸§çš„flagéƒ½è®¾ç½®ä¸º0ã€‚
    """
    if not actions:
        return []
    n_actions = len(actions)
    flags = [1] * n_actions  # é»˜è®¤å…¨éƒ¨ä¸º1
    for i in range(n_actions - 2, -1, -1):  # ä»å€’æ•°ç¬¬äºŒå¸§å¾€å‰
        curr = np.array(actions[i])
        next_ = np.array(actions[i + 1])
        diff = np.linalg.norm(curr - next_)
        if diff > threshold:
            # å½“å‰åŠä¹‹å‰æ‰€æœ‰å¸§éƒ½ç½®ä¸º0
            for j in range(i + 1):
                flags[j] = 0
            break
    return flags

def update_action_stats(stats, terminated_flags):
    """
    æ›´æ–°actionç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ºæ–°æ·»åŠ çš„ç»ˆæ­¢æ ‡å¿—ç»´åº¦æ·»åŠ ç»Ÿè®¡æ•°æ®ã€‚
    """
    arr = np.array(terminated_flags)
    stats['min'].append(int(arr.min()) if arr.size > 0 else 0)
    stats['max'].append(int(arr.max()) if arr.size > 0 else 0)
    stats['mean'].append(float(arr.mean()) if arr.size > 0 else 0.0)
    stats['std'].append(float(arr.std()) if arr.size > 0 else 0.0)
    return stats

def process_parquet_file(src, dst, threshold=5.0):
    """
    è¯»å–parquetæ–‡ä»¶ï¼Œä¸ºactionæ·»åŠ ç»ˆæ­¢æ ‡å¿—ç»´åº¦ï¼Œå¹¶ä¿å­˜åˆ°æ–°è·¯å¾„ã€‚
    """
    df = pd.read_parquet(src)
    
    if 'action' not in df.columns or df['action'].empty:
        shutil.copy2(src, dst)
        return []

    actions = df['action'].tolist()

    if not isinstance(actions[0], (list, np.ndarray)) or len(actions[0]) != 6:
        shutil.copy2(src, dst)
        return []

    terminated_flags = calc_terminated_flag(actions, threshold)
    
    new_actions = [list(original_action) + [flag] for original_action, flag in zip(actions, terminated_flags)]

    output_df = df.copy()
    output_df['action'] = new_actions

    if len(output_df['action'].iloc[0]) != 7:
        raise RuntimeError(f"å¤„ç†æ–‡ä»¶ {src} åï¼Œactionç»´åº¦é”™è¯¯ï¼Œåº”ä¸º7ã€‚")

    output_df.to_parquet(dst, index=False)
    
    return terminated_flags

def process_stats_line(line, terminated_flags):
    """
    å¤„ç†å•è¡Œepisodes_stats.jsonlæ•°æ®ã€‚
    """
    data = json.loads(line)
    if 'action' in data.get('stats', {}):
        stats = data['stats']['action']
        if len(stats['min']) == 6:
            update_action_stats(stats, terminated_flags)
    return json.dumps(data, ensure_ascii=False)

def update_info_json(src_file, dst_file):
    """
    è¯»å–info.jsonï¼Œæ›´æ–°actionçš„shapeå’Œnamesï¼Œå¹¶ä¿å­˜åˆ°æ–°è·¯å¾„ã€‚
    """
    if not src_file.exists():
        print("    - âš ï¸  è­¦å‘Š: info.json ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ã€‚")
        return

    with open(src_file, 'r', encoding='utf-8') as f:
        info_data = json.load(f)

    if 'features' in info_data and 'action' in info_data['features']:
        action_feature = info_data['features']['action']
        
        if action_feature.get('shape') == [6]:
            action_feature['shape'] = [7]
        if 'names' in action_feature and 'flag' not in action_feature['names']:
            action_feature['names'].append('flag')

    with open(dst_file, 'w', encoding='utf-8') as f:
        json.dump(info_data, f, indent=2, ensure_ascii=False)
    print("    - info.json æ›´æ–°å®Œæˆã€‚")

def update_modality_json(src_file, dst_file):
    """
    è¯»å–modality.jsonï¼Œä¸ºactionæ·»åŠ flagæ¡ç›®ï¼Œå¹¶ä¿å­˜åˆ°æ–°è·¯å¾„ã€‚
    """
    if not src_file.exists():
        print("    - âš ï¸  è­¦å‘Š: modality.json ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ã€‚")
        return

    with open(src_file, 'r', encoding='utf-8') as f:
        modality_data = json.load(f)

    if 'action' in modality_data and 'flag' not in modality_data['action']:
        modality_data['action']['flag'] = {"start": 6, "end": 7}

    with open(dst_file, 'w', encoding='utf-8') as f:
        json.dump(modality_data, f, indent=4, ensure_ascii=False)
    print("    - modality.json æ›´æ–°å®Œæˆã€‚")


# ==============================================================================
# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° (åˆå¹¶åçš„) ---
# ==============================================================================

def find_dataset_folders(base_path):
    """
    åœ¨æŒ‡å®šçš„åŸºç¡€è·¯å¾„ä¸‹é€’å½’æŸ¥æ‰¾æ‰€æœ‰çš„æ•°æ®é›†æ–‡ä»¶å¤¹ã€‚
    ä¸€ä¸ªâ€œæ•°æ®é›†æ–‡ä»¶å¤¹â€è¢«å®šä¹‰ä¸ºåŒæ—¶åŒ…å« 'videos', 'meta', å’Œ 'data' è¿™ä¸‰ä¸ªå­æ–‡ä»¶å¤¹çš„ç›®å½•ã€‚
    """
    required_subdirs = {'videos', 'meta', 'data'}
    dataset_paths = []
    print(f"\nğŸ” å¼€å§‹åœ¨ '{os.path.abspath(base_path)}' ä¸­æœç´¢æ•°æ®é›†...\n")
    for root, dirs, _ in os.walk(base_path):
        dir_set = set(dirs)
        if required_subdirs.issubset(dir_set):
            dataset_path = Path(root)
            dataset_paths.append(dataset_path)
            print(f"  [âœ… æ‰¾åˆ°!] -> {dataset_path}")
            # é˜²æ­¢é‡å¤æŸ¥æ‰¾å­ç›®å½•ä¸­çš„æ•°æ®é›†
            dirs[:] = [d for d in dirs if d not in required_subdirs]
    return dataset_paths

def process_single_dataset(src_path: Path, dst_path: Path, threshold: float):
    """
    å¯¹å•ä¸ªæºæ•°æ®é›†è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°ç›®æ ‡è·¯å¾„ã€‚
    """
    if not src_path.is_dir():
        print(f"    - âŒ é”™è¯¯: è¾“å…¥è·¯å¾„ {src_path} ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•ã€‚")
        return

    # 1. å¤åˆ¶ videos æ–‡ä»¶å¤¹
    src_videos = src_path / 'videos'
    if src_videos.is_dir():
        shutil.copytree(src_videos, dst_path / 'videos', dirs_exist_ok=True)
        print("    - 'videos' æ–‡ä»¶å¤¹å·²å¤åˆ¶ã€‚")

    # 2. å¤„ç† data æ–‡ä»¶å¤¹
    src_chunk_dir = src_path / 'data' / 'chunk-000'
    dst_chunk_dir = dst_path / 'data' / 'chunk-000'
    dst_chunk_dir.mkdir(parents=True, exist_ok=True)
    
    episode_flags = {}
    if src_chunk_dir.is_dir():
        print("    - æ­£åœ¨å¤„ç† parquet æ–‡ä»¶...")
        for parquet_file in sorted(src_chunk_dir.glob('episode_*.parquet')):
            dst_file = dst_chunk_dir / parquet_file.name
            flags = process_parquet_file(parquet_file, dst_file, threshold)
            if flags:
                ep_idx = int(parquet_file.stem.split('_')[1])
                episode_flags[ep_idx] = flags
        print("    - Parquet æ–‡ä»¶å¤„ç†å®Œæˆã€‚")

    # 3. å¤„ç† meta æ–‡ä»¶å¤¹
    src_meta_dir = src_path / 'meta'
    dst_meta_dir = dst_path / 'meta'
    dst_meta_dir.mkdir(parents=True, exist_ok=True)

    if src_meta_dir.is_dir():
        # å¤åˆ¶ä¸éœ€ä¿®æ”¹çš„æ–‡ä»¶
        for fn in ['tasks.jsonl', 'episodes.jsonl']:
            if (src_meta_dir / fn).exists():
                shutil.copy2(src_meta_dir / fn, dst_meta_dir / fn)

        # æ›´æ–° info.json, modality.json, episodes_stats.jsonl
        update_info_json(src_meta_dir / 'info.json', dst_meta_dir / 'info.json')
        update_modality_json(src_meta_dir / 'modality.json', dst_meta_dir / 'modality.json')
        
        src_stats_file = src_meta_dir / 'episodes_stats.jsonl'
        dst_stats_file = dst_meta_dir / 'episodes_stats.jsonl'
        if src_stats_file.exists():
            print("    - æ­£åœ¨æ›´æ–° episodes_stats.jsonl...")
            with open(src_stats_file, 'r', encoding='utf-8') as fin, open(dst_stats_file, 'w', encoding='utf-8') as fout:
                for line in fin:
                    data = json.loads(line)
                    ep_idx = data.get('episode_index')
                    if ep_idx is not None and ep_idx in episode_flags:
                        new_line = process_stats_line(line, episode_flags[ep_idx])
                        fout.write(new_line + '\n')
                    else:
                        fout.write(line)
            print("    - episodes_stats.jsonl æ›´æ–°å®Œæˆã€‚")

def main():
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨åŒ–æŸ¥æ‰¾å¹¶å¤„ç† LeRobot æ•°æ®é›†ï¼Œä¸º action æ·»åŠ ç»ˆæ­¢æ ‡å¿—ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "--src_base_path", type=str, required=True,
        help="è¦å¼€å§‹æœç´¢çš„æºæ ¹ç›®å½•è·¯å¾„ã€‚\nä¾‹å¦‚: /path/to/raw_data"
    )
    parser.add_argument(
        "--dst_base_path", type=str, required=True,
        help="ç”¨äºå­˜å‚¨å¤„ç†åæ•°æ®é›†çš„ç›®æ ‡æ ¹ç›®å½•è·¯å¾„ã€‚\nå°†ä¿æŒä¸æºç›®å½•ç›¸åŒçš„å­ç›®å½•ç»“æ„ã€‚\nä¾‹å¦‚: /path/to/processed_data"
    )
    parser.add_argument(
        "--search_dirs", type=str, default="*",
        help="åœ¨ src_base_path ä¸‹è¦æœç´¢çš„å­ç›®å½•ï¼Œç”¨é€—å·åˆ†éš”ã€‚\næ”¯æŒé€šé…ç¬¦ '*'ã€‚é»˜è®¤ä¸º '*' (æœç´¢æ‰€æœ‰å­ç›®å½•)ã€‚\nä¾‹å¦‚: blk0,blk3,task_*"
    )
    parser.add_argument(
        "--threshold", type=float, default=3.0,
        help="ç”¨äºåˆ¤æ–­ç»ˆæ­¢çš„ action å·®å¼‚é˜ˆå€¼ã€‚é»˜è®¤: 3.0"
    )
    
    args = parser.parse_args()

    # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®é›†
    search_dirs = [d.strip() for d in args.search_dirs.split(',')]
    all_found_datasets = []
    for directory in search_dirs:
        search_path_pattern = Path(args.src_base_path) / directory
        if '*' not in directory and '?' not in directory:
            if search_path_pattern.is_dir():
                all_found_datasets.extend(find_dataset_folders(search_path_pattern))
        else:
            for matching_dir in Path(args.src_base_path).glob(directory):
                if matching_dir.is_dir():
                    all_found_datasets.extend(find_dataset_folders(matching_dir))
    
    if not all_found_datasets:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„æ•°æ®é›†æ–‡ä»¶å¤¹ã€‚è¯·æ£€æŸ¥ --src_base_path å’Œ --search_dirs å‚æ•°ã€‚")
        return
        
    print(f"\nâœ¨ æ€»å…±æ‰¾åˆ° {len(all_found_datasets)} ä¸ªæ•°æ®é›†ï¼Œå³å°†å¼€å§‹å¤„ç†...\n" + "="*80)

    processed_count = 0
    skipped_count = 0

    for i, src_dataset_path in enumerate(all_found_datasets):
        print(f"\n({i+1}/{len(all_found_datasets)}) æ£€æŸ¥æ•°æ®é›†: {src_dataset_path}")
        print("-" * 60)

        # è®¡ç®—ç›®æ ‡è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
        relative_path = src_dataset_path.relative_to(args.src_base_path)
        dst_dataset_path = Path(args.dst_base_path) / relative_path

        # è·³è¿‡ç‰¹æ®Šæˆ–å·²å­˜åœ¨çš„ç›®æ ‡
        if 'merged' in str(src_dataset_path):
            print(f"    - [â¡ï¸ è·³è¿‡] åŸå› : æ•°æ®é›†åç§°åŒ…å« 'merged'ã€‚")
            skipped_count += 1
            continue
        if dst_dataset_path.exists():
            print(f"    - [â¡ï¸ è·³è¿‡] åŸå› : ç›®æ ‡è·¯å¾„ {dst_dataset_path} å·²å­˜åœ¨ã€‚")
            skipped_count += 1
            continue
        
        print(f"    - [âš™ï¸ å¤„ç†ä¸­] -> è¾“å‡ºåˆ°: {dst_dataset_path}")
        
        try:
            process_single_dataset(src_dataset_path, dst_dataset_path, args.threshold)
            processed_count += 1
            print(f"    - [âœ… å®Œæˆ]")
        except Exception as e:
            print(f"    - [âŒ å¤±è´¥] å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œå¯ä»¥é€‰æ‹©åˆ é™¤ä¸å®Œæ•´çš„è¾“å‡ºç›®å½•
            if dst_dataset_path.exists():
                print(f"    - æ­£åœ¨æ¸…ç†ä¸å®Œæ•´çš„è¾“å‡ºç›®å½•: {dst_dataset_path}")
                shutil.rmtree(dst_dataset_path)


    print("\n" + "="*80)
    print("ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    print(f"   - æ€»å…±æ‰¾åˆ°æ•°æ®é›†: {len(all_found_datasets)}")
    print(f"   - æˆåŠŸå¤„ç†æ•°æ®é›†: {processed_count}")
    print(f"   - è·³è¿‡çš„æ•°æ®é›†:   {skipped_count}")
    print("="*80)


if __name__ == "__main__":
    main()