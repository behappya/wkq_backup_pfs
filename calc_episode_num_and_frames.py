# calculate_dataset_stats.py

import os
import argparse
import json
from pathlib import Path

# --- å¸®åŠ©å‡½æ•° ---

def load_jsonl(path):
    """åŠ è½½ä¸€ä¸ª JSONL æ–‡ä»¶ã€‚"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return [json.loads(l) for l in f if l.strip()]
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"    - âš ï¸  è¯»å–æˆ–è§£ææ–‡ä»¶ {path} æ—¶å‡ºé”™: {e}")
        return []

# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

def find_dataset_folders(base_path):
    """
    åœ¨æŒ‡å®šçš„åŸºç¡€è·¯å¾„ä¸‹é€’å½’æŸ¥æ‰¾æ‰€æœ‰çš„æ•°æ®é›†æ–‡ä»¶å¤¹ã€‚
    ä¸€ä¸ªâ€œæ•°æ®é›†æ–‡ä»¶å¤¹â€è¢«å®šä¹‰ä¸ºåŒæ—¶åŒ…å« 'videos', 'meta', å’Œ 'data' è¿™ä¸‰ä¸ªå­æ–‡ä»¶å¤¹çš„ç›®å½•ã€‚
    """
    required_subdirs = {'videos', 'meta', 'data'}
    dataset_paths = []
    print(f"\nğŸ” å¼€å§‹åœ¨ '{os.path.abspath(base_path)}' ä¸­æœç´¢æ•°æ®é›†...\n")
    for root, dirs, _ in os.walk(base_path):
        dir_set = set(dirs)
        if required_subdirs.issubset(dir_set):
            dataset_path = Path(root)
            dataset_paths.append(dataset_path)
            print(f"  [âœ… æ‰¾åˆ°!] -> {dataset_path}")
    return dataset_paths

def calculate_stats_for_dataset(dataset_path: Path):
    """
    è®¡ç®—å•ä¸ªæ•°æ®é›†çš„ episode æ•°é‡å’Œæ€»å¸§æ•°ã€‚
    """
    episodes_path = dataset_path / "meta" / "episodes.jsonl"
    
    if not episodes_path.exists():
        print(f"    - âŒ é”™è¯¯: æ‰¾ä¸åˆ°å…ƒæ•°æ®æ–‡ä»¶ {episodes_path}ã€‚")
        return 0, 0

    episodes_data = load_jsonl(episodes_path)
    
    num_episodes = len(episodes_data)
    total_frames = 0
    
    for episode in episodes_data:
        # ä½¿ç”¨ .get() æ–¹æ³•å®‰å…¨åœ°è·å– 'length' é”®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸º 0
        total_frames += episode.get('length', 0)
        
    return num_episodes, total_frames


def main():
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨åŒ–æŸ¥æ‰¾ LeRobot æ•°æ®é›†å¹¶ç»Ÿè®¡å…¶æ€» episode æ•°é‡å’Œæ€»å¸§æ•°ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "--src_base_path", type=str, required=True,
        help="è¦å¼€å§‹æœç´¢çš„æºæ ¹ç›®å½•è·¯å¾„ã€‚\nä¾‹å¦‚: /pfs/pfs-ahGxdf/data/collect_data/so101"
    )
    parser.add_argument(
        "--search_dirs", type=str, default="*",
        help="åœ¨ src_base_path ä¸‹è¦æœç´¢çš„å­ç›®å½•ï¼Œç”¨é€—å·åˆ†éš”ã€‚\næ”¯æŒé€šé…ç¬¦ '*'ã€‚é»˜è®¤ä¸º '*' (æœç´¢æ‰€æœ‰å­ç›®å½•)ã€‚\nä¾‹å¦‚: blk0,blk3"
    )
    
    args = parser.parse_args()

    # å°†é€—å·åˆ†éš”çš„æœç´¢ç›®å½•è½¬æ¢ä¸ºåˆ—è¡¨
    search_dirs = [d.strip() for d in args.search_dirs.split(',')]

    all_found_datasets = []
    for directory in search_dirs:
        # ä½¿ç”¨ Path.glob æ”¯æŒé€šé…ç¬¦
        search_path_pattern = Path(args.src_base_path) / directory
        # glob() ä¸ä¼šç›´æ¥è¿”å›è·¯å¾„æœ¬èº«ï¼Œæ‰€ä»¥å¦‚æœä¸æ˜¯é€šé…ç¬¦æ¨¡å¼ï¼Œéœ€è¦å•ç‹¬å¤„ç†
        if '*' not in directory and '?' not in directory:
             if search_path_pattern.is_dir():
                all_found_datasets.extend(find_dataset_folders(search_path_pattern))
        else:
            # æœç´¢åŒ¹é…é€šé…ç¬¦çš„ç›®å½•
            for matching_dir in Path(args.src_base_path).glob(directory):
                if matching_dir.is_dir():
                    all_found_datasets.extend(find_dataset_folders(matching_dir))
    
    if not all_found_datasets:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„æ•°æ®é›†æ–‡ä»¶å¤¹ã€‚è¯·æ£€æŸ¥ --src_base_path å’Œ --search_dirs å‚æ•°ã€‚")
        return
        
    print(f"\nâœ¨ æ€»å…±æ‰¾åˆ° {len(all_found_datasets)} ä¸ªæ•°æ®é›†ï¼Œå³å°†å¼€å§‹ç»Ÿè®¡...\n" + "="*80)

    grand_total_episodes = 0
    grand_total_frames = 0

    for i, src_path in enumerate(all_found_datasets):
        if 'merged' in str(src_path):
            print(f"\n({i+1}/{len(all_found_datasets)}) è·³è¿‡åˆå¹¶æ•°æ®é›†: {src_path}")
            continue
        print(f"\n({i+1}/{len(all_found_datasets)}) æ­£åœ¨ç»Ÿè®¡: {src_path}")
        print("-" * 60)

        num_episodes, total_frames = calculate_stats_for_dataset(src_path)
        
        if num_episodes > 0 or total_frames > 0:
            print(f"    - æœ¬æ•°æ®é›† Episode æ•°é‡: {num_episodes}")
            print(f"    - æœ¬æ•°æ®é›†æ€»å¸§æ•°: {total_frames}")
            grand_total_episodes += num_episodes
            grand_total_frames += total_frames
        else:
            print(f"    - æœªèƒ½ä»æ­¤æ•°æ®é›†ä¸­ç»Ÿè®¡åˆ°æœ‰æ•ˆæ•°æ®ã€‚")


    print("\n" + "="*80)
    print("ğŸ‰ ç»Ÿè®¡å®Œæˆï¼")
    print(f"   - æ€»å…±æ‰«ææ•°æ®é›†æ•°é‡: {len(all_found_datasets)}")
    print(f"   - æ‰€æœ‰æ•°æ®é›†æ€» Episode æ•°é‡: {grand_total_episodes}")
    print(f"   - æ‰€æœ‰æ•°æ®é›†æ€»å¸§æ•° (Total Frames): {grand_total_frames}")
    print("="*80)


if __name__ == "__main__":
    main()