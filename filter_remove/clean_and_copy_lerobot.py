# import argparse
# from pathlib import Path

# ä»å‚æ•°ä¸­è¯»å–ç›¸æœº
import argparse
from pathlib import Path
import json
import shutil
import pandas as pd


def load_jsonl(path):
    with open(path, 'r') as f:
        return [json.loads(l) for l in f if l.strip()]


def save_jsonl(path, lines):
    with open(path, 'w') as f:
        for line in lines:
            f.write(json.dumps(line) + '\n')


def main(args):
    # è·¯å¾„è®¾ç½®
    src_root = Path(args.src_root)
    dst_root = Path(args.dst_root)
    remove_txt = Path(args.remove_txt)
    cam_list = [cam.strip() for cam in args.cams.split(",") if cam.strip()]

    # æºè·¯å¾„
    src_data = src_root / "data/chunk-000"
    src_videos = {cam: src_root / f"videos/chunk-000/observation.images.{cam}" for cam in cam_list}
    src_meta = src_root / "meta"

    # ç›®æ ‡è·¯å¾„
    dst_data = dst_root / "data/chunk-000"
    dst_videos = {cam: dst_root / f"videos/chunk-000/observation.images.{cam}" for cam in cam_list}
    dst_meta = dst_root / "meta"

    # åˆ›å»ºç›®æ ‡ç›®å½•
    for p in [dst_data, dst_meta] + list(dst_videos.values()):
        p.mkdir(parents=True, exist_ok=True)

    # åŠ è½½éœ€è¦åˆ é™¤çš„ episode id åˆ—è¡¨
    with open(remove_txt, "r") as f:
        remove_ids = set(f"{int(line.strip()):06d}" for line in f if line.strip())

    # åŠ è½½ meta æ–‡ä»¶
    episodes = load_jsonl(src_meta / "episodes.jsonl")
    stats = load_jsonl(src_meta / "episodes_stats.jsonl")

    # ä¿ç•™æœªåˆ é™¤çš„ entries
    filtered = [
        (ep, st) for ep, st in zip(episodes, stats)
        if f"{ep['episode_index']:06d}" not in remove_ids
    ]

    # æŒ‰é¡ºåºå¤„ç†å‰©ä¸‹çš„ episode
    for new_idx, (ep, st) in enumerate(filtered):
        old_idx_str = f"{ep['episode_index']:06d}"
        new_idx_str = f"{new_idx:06d}"

        # æ›´æ–° JSON ä¸­çš„ episode_index å­—æ®µ
        ep["episode_index"] = new_idx
        st["episode_index"] = new_idx

        # === ä¿®æ”¹ parquet ä¸­çš„ episode_index å­—æ®µ ===
        old_parquet = src_data / f"episode_{old_idx_str}.parquet"
        new_parquet = dst_data / f"episode_{new_idx_str}.parquet"
        if old_parquet.exists():
            df = pd.read_parquet(old_parquet)
            if "episode_index" in df.columns:
                df["episode_index"] = new_idx
            else:
                print(f"âš ï¸ Warning: 'episode_index' not found in {old_parquet.name}")
            df.to_parquet(new_parquet)
            print(f"âœ”ï¸ Saved {new_parquet.name} with updated episode_index = {new_idx}")

        # === æ‹·è´å¯¹åº”è§†é¢‘æ–‡ä»¶ï¼ˆä¸åšä¿®æ”¹ï¼‰ ===
        for cam in cam_list:
            cam_src = src_videos[cam]
            cam_dst = dst_videos[cam]
            old_mp4 = cam_src / f"episode_{old_idx_str}.mp4"
            new_mp4 = cam_dst / f"episode_{new_idx_str}.mp4"
            if old_mp4.exists():
                shutil.copy2(old_mp4, new_mp4)

    # === ä¿å­˜æ›´æ–°åçš„ meta æ–‡ä»¶ ===
    save_jsonl(dst_meta / "episodes.jsonl", [ep for ep, _ in filtered])
    save_jsonl(dst_meta / "episodes_stats.jsonl", [st for _, st in filtered])

    # å°†modalit.jsonä¹Ÿcopyè¿‡å»
    shutil.copy2(args.modality_file_path, dst_meta / "modality.json")


    # å°†åŸæ¥metaä¸‹é¢çš„tasks.jsonlä¹Ÿcopyè¿‡å»
    shutil.copy2(src_meta / "tasks.jsonl", dst_meta / "tasks.jsonl")

    # === æ›´æ–° info.json ä¸­ total_episodes å­—æ®µ ===
    info_path_src = src_meta / "info.json"
    info_path_dst = dst_meta / "info.json"
    with open(info_path_src, 'r') as f:
        info = json.load(f)
    info["total_episodes"] = len(filtered)
    info["total_videos"] = len(cam_list) * len(filtered)
    info["splits"]["train"] = "0:" + str(len(filtered))
    with open(info_path_dst, 'w') as f:
        json.dump(info, f, indent=2)

    print(f"\nâœ… æ¸…ç†å’Œå¤åˆ¶å®Œæˆï¼å…±ä¿ç•™ {len(filtered)} ä¸ª episodesï¼Œç¼–å·ä» 000000 å¼€å§‹ã€‚")
    print(f"ğŸ“ è¾“å‡ºä¿å­˜è·¯å¾„: {dst_root}")
    print("è¯·å†æ¬¡æ£€æŸ¥åˆ é™¤åtasks.jsonlçš„æ˜ å°„æ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Clean and copy LeRobot dataset with episode_index updated")
    parser.add_argument("--src_root", type=str, required=True, help="Path to source LeRobot folder")
    parser.add_argument("--dst_root", type=str, required=True, help="Path to output folder")
    parser.add_argument("--remove_txt", type=str, required=True, help="Path to txt file containing episode IDs to remove")
    parser.add_argument("--cams", type=str, required=True, help="Comma-separated list of camera suffixes, e.g. 'front,wrist,side'")
    parser.add_argument("--modality_file_path", type=str, required=True, help="Path to modality.json file")
    args = parser.parse_args()
    main(args)

#===ori
# import json
# import shutil
# import pandas as pd


# def load_jsonl(path):
#     with open(path, 'r') as f:
#         return [json.loads(l) for l in f if l.strip()]


# def save_jsonl(path, lines):
#     with open(path, 'w') as f:
#         for line in lines:
#             f.write(json.dumps(line) + '\n')


# def main(args):
#     # è·¯å¾„è®¾ç½®
#     src_root = Path(args.src_root)
#     dst_root = Path(args.dst_root)
#     remove_txt = Path(args.remove_txt)

#     # æºè·¯å¾„
#     src_data = src_root / "data/chunk-000"
#     src_front = src_root / "videos/chunk-000/observation.images.front"
#     src_wrist = src_root / "videos/chunk-000/observation.images.wrist"
#     src_meta = src_root / "meta"

#     # ç›®æ ‡è·¯å¾„
#     dst_data = dst_root / "data/chunk-000"
#     dst_front = dst_root / "videos/chunk-000/observation.images.front"
#     dst_wrist = dst_root / "videos/chunk-000/observation.images.wrist"
#     dst_meta = dst_root / "meta"

#     # åˆ›å»ºç›®æ ‡ç›®å½•
#     for p in [dst_data, dst_front, dst_wrist, dst_meta]:
#         p.mkdir(parents=True, exist_ok=True)

#     # åŠ è½½éœ€è¦åˆ é™¤çš„ episode id åˆ—è¡¨
#     with open(remove_txt, "r") as f:
#         remove_ids = set(line.strip() for line in f if line.strip())

#     # åŠ è½½ meta æ–‡ä»¶
#     episodes = load_jsonl(src_meta / "episodes.jsonl")
#     stats = load_jsonl(src_meta / "episodes_stats.jsonl")

#     # ä¿ç•™æœªåˆ é™¤çš„ entries
#     filtered = [
#         (ep, st) for ep, st in zip(episodes, stats)
#         if f"{ep['episode_index']:06d}" not in remove_ids
#     ]

#     # æŒ‰é¡ºåºå¤„ç†å‰©ä¸‹çš„ episode
#     for new_idx, (ep, st) in enumerate(filtered):
#         old_idx_str = f"{ep['episode_index']:06d}"
#         new_idx_str = f"{new_idx:06d}"

#         # æ›´æ–° JSON ä¸­çš„ episode_index å­—æ®µ
#         ep["episode_index"] = new_idx
#         st["episode_index"] = new_idx

#         # === ä¿®æ”¹ parquet ä¸­çš„ episode_index å­—æ®µ ===
#         old_parquet = src_data / f"episode_{old_idx_str}.parquet"
#         new_parquet = dst_data / f"episode_{new_idx_str}.parquet"
#         if old_parquet.exists():
#             df = pd.read_parquet(old_parquet)
#             if "episode_index" in df.columns:
#                 df["episode_index"] = new_idx
#             else:
#                 print(f"âš ï¸ Warning: 'episode_index' not found in {old_parquet.name}")
#             df.to_parquet(new_parquet)
#             print(f"âœ”ï¸ Saved {new_parquet.name} with updated episode_index = {new_idx}")

#         # === æ‹·è´å¯¹åº”è§†é¢‘æ–‡ä»¶ï¼ˆä¸åšä¿®æ”¹ï¼‰ ===
#         for cam_src, cam_dst in [(src_front, dst_front), (src_wrist, dst_wrist)]:
#             old_mp4 = cam_src / f"episode_{old_idx_str}.mp4"
#             new_mp4 = cam_dst / f"episode_{new_idx_str}.mp4"
#             if old_mp4.exists():
#                 shutil.copy2(old_mp4, new_mp4)

#     # === ä¿å­˜æ›´æ–°åçš„ meta æ–‡ä»¶ ===
#     save_jsonl(dst_meta / "episodes.jsonl", [ep for ep, _ in filtered])
#     save_jsonl(dst_meta / "episodes_stats.jsonl", [st for _, st in filtered])

#     # === æ›´æ–° info.json ä¸­ total_episodes å­—æ®µ ===
#     info_path_src = src_meta / "info.json"
#     info_path_dst = dst_meta / "info.json"
#     with open(info_path_src, 'r') as f:
#         info = json.load(f)
#     info["total_episodes"] = len(filtered)
#     with open(info_path_dst, 'w') as f:
#         json.dump(info, f, indent=2)

#     print(f"\nâœ… æ¸…ç†å’Œå¤åˆ¶å®Œæˆï¼å…±ä¿ç•™ {len(filtered)} ä¸ª episodesï¼Œç¼–å·ä» 000000 å¼€å§‹ã€‚")
#     print(f"ğŸ“ è¾“å‡ºä¿å­˜è·¯å¾„: {dst_root}")
#     raise NotImplementedError("è¯·æ‰‹åŠ¨ä¿®æ”¹info.jsonä»¥åŠæ·»åŠ task.json")


# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="Clean and copy LeRobot dataset with episode_index updated")
#     parser.add_argument("--src_root", type=str, required=True, help="Path to source LeRobot folder")
#     parser.add_argument("--dst_root", type=str, required=True, help="Path to output folder")
#     parser.add_argument("--remove_txt", type=str, required=True, help="Path to txt file containing episode IDs to remove")

#     args = parser.parse_args()
#     main(args)
