# process_datasets.py

import os
import argparse
import json
import shutil
import pandas as pd
from pathlib import Path
import subprocess
import sys

# --- å¸®åŠ©å‡½æ•° (æ¥è‡ª clean_and_copy_lerobot.py) ---

def load_jsonl(path):
    """åŠ è½½ä¸€ä¸ª JSONL æ–‡ä»¶ã€‚"""
    with open(path, 'r') as f:
        return [json.loads(l) for l in f if l.strip()]

def save_jsonl(path, lines):
    """ä¿å­˜æ•°æ®åˆ° JSONL æ–‡ä»¶ã€‚"""
    with open(path, 'w') as f:
        for line in lines:
            f.write(json.dumps(line) + '\n')

# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

def find_dataset_folders(base_path):
    """
    åœ¨æŒ‡å®šçš„åŸºç¡€è·¯å¾„ä¸‹é€’å½’æŸ¥æ‰¾æ‰€æœ‰çš„æ•°æ®é›†æ–‡ä»¶å¤¹ã€‚
    ä¸€ä¸ªâ€œæ•°æ®é›†æ–‡ä»¶å¤¹â€è¢«å®šä¹‰ä¸ºåŒæ—¶åŒ…å« 'videos', 'meta', å’Œ 'data' è¿™ä¸‰ä¸ªå­æ–‡ä»¶å¤¹çš„ç›®å½•ã€‚
    """
    required_subdirs = {'videos', 'meta', 'data'}
    dataset_paths = []
    print(f"\nğŸ” å¼€å§‹åœ¨ '{os.path.abspath(base_path)}' ä¸­æœç´¢æ•°æ®é›†...\n")
    for root, dirs, _ in os.walk(base_path):
        dir_set = set(dirs)
        if required_subdirs.issubset(dir_set):
            dataset_paths.append(Path(root))
            print(f"  [âœ… æ‰¾åˆ°!] -> {root}")
    return dataset_paths

def run_video_validation(dataset_path: Path, validator_script_path: str):
    """
    è¿è¡Œå¤–éƒ¨è§†é¢‘éªŒè¯è„šæœ¬æ¥ç”Ÿæˆ low_quality.txtã€‚
    å¦‚æœéªŒè¯è„šæœ¬è·¯å¾„ä¸ºç©ºï¼Œåˆ™åªåˆ›å»ºä¸€ä¸ªç©ºçš„ low_quality.txtã€‚
    """
    output_txt = dataset_path / "low_quality.txt"
    print(f"  STEP 1: è¿è¡Œè§†é¢‘è´¨æ£€...")
    if validator_script_path and Path(validator_script_path).exists():
        try:
            print(f"    - æ‰§è¡Œè„šæœ¬: python {validator_script_path} {dataset_path}")
            # æ³¨æ„ï¼šæ­¤å‘½ä»¤ä¼šè¦†ç›–ç°æœ‰çš„ low_quality.txt
            subprocess.run(
                [sys.executable, validator_script_path, str(dataset_path)],
                check=True,
                capture_output=True,
                text=True
            )
            print(f"    - è§†é¢‘è´¨æ£€å®Œæˆ, ç»“æœä¿å­˜åœ¨: {output_txt}")
        except subprocess.CalledProcessError as e:
            print(f"    - âš ï¸ è§†é¢‘è´¨æ£€è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
            print(f"    - STDOUT: {e.stdout}")
            print(f"    - STDERR: {e.stderr}")
            print(f"    - å°†åˆ›å»ºä¸€ä¸ªç©ºçš„ low_quality.txt æ–‡ä»¶ç»§ç»­æ‰§è¡Œã€‚")
            output_txt.touch() # åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ä»¥é˜²è„šæœ¬å¤±è´¥
    else:
        print(f"    - æœªæä¾›æœ‰æ•ˆçš„è´¨æ£€è„šæœ¬è·¯å¾„ï¼Œå°†åˆ›å»ºä¸€ä¸ªç©ºçš„ low_quality.txtã€‚")
        output_txt.touch() # åˆ›å»ºç©ºæ–‡ä»¶
    return output_txt

def combine_manual_removals(remove_txt_path: Path, manual_ids_str: str):
    """
    å°†æ‰‹åŠ¨æŒ‡å®šçš„ IDs è¿½åŠ åˆ°è‡ªåŠ¨ç”Ÿæˆçš„ remove_txt æ–‡ä»¶ä¸­ã€‚
    """
    if not manual_ids_str:
        print("  STEP 2: æ— éœ€æ·»åŠ æ‰‹åŠ¨ç§»é™¤çš„ episodesã€‚")
        return

    print(f"  STEP 2: æ·»åŠ æ‰‹åŠ¨æŒ‡å®šçš„ç§»é™¤ IDs: {manual_ids_str}")
    # è¯»å–å·²æœ‰çš„ IDs
    existing_ids = set()
    if remove_txt_path.exists():
        with open(remove_txt_path, "r") as f:
            existing_ids = set(line.strip() for line in f if line.strip())

    # è§£æå¹¶æ·»åŠ æ–°çš„ IDs
    manual_ids = {id.strip() for id in manual_ids_str.split(',') if id.strip()}
    all_ids = sorted(list(existing_ids.union(manual_ids)), key=int)

    # å†™å›æ–‡ä»¶
    with open(remove_txt_path, "w") as f:
        for episode_id in all_ids:
            f.write(f"{episode_id}\n")
    print(f"    - æˆåŠŸåˆå¹¶ç§»é™¤åˆ—è¡¨åˆ°: {remove_txt_path}")


def clean_and_copy_dataset(src_root: Path, dst_root: Path, remove_txt: Path, cams: str, modality_file_path: Path):
    """
    æ¸…ç†å¹¶å¤åˆ¶å•ä¸ª LeRobot æ•°æ®é›†ï¼ŒåŒæ—¶æ›´æ–° episode_indexã€‚
    è¿™æ˜¯ `clean_and_copy_lerobot.py` çš„æ ¸å¿ƒé€»è¾‘ã€‚
    """
    print(f"  STEP 3: å¼€å§‹æ¸…ç†å’Œå¤åˆ¶...")
    print(f"    - æºè·¯å¾„: {src_root}")
    print(f"    - ç›®æ ‡è·¯å¾„: {dst_root}")

    cam_list = [cam.strip() for cam in cams.split(",") if cam.strip()]

    # æºè·¯å¾„
    src_data = src_root / "data/chunk-000"
    src_videos = {cam: src_root / f"videos/chunk-000/observation.images.{cam}" for cam in cam_list}
    src_meta = src_root / "meta"

    # ç›®æ ‡è·¯å¾„
    dst_data = dst_root / "data/chunk-000"
    dst_videos = {cam: dst_root / f"videos/chunk-000/observation.images.{cam}" for cam in cam_list}
    dst_meta = dst_root / "meta"

    # åˆ›å»ºç›®æ ‡ç›®å½•
    for p in [dst_data, dst_meta] + list(dst_videos.values()):
        p.mkdir(parents=True, exist_ok=True)

    # åŠ è½½éœ€è¦åˆ é™¤çš„ episode id åˆ—è¡¨
    remove_ids = set()
    if remove_txt.exists():
        with open(remove_txt, "r") as f:
            # æ ¼å¼åŒ–ä¸º6ä½è¡¥é›¶å­—ç¬¦ä¸²ä»¥ä¾¿åŒ¹é…
            remove_ids = set(f"{int(line.strip()):06d}" for line in f if line.strip())
        print(f"    - å°†ç§»é™¤ {len(remove_ids)} ä¸ª episodes: {sorted(list(remove_ids))}")
    else:
        print(f"    - æœªæ‰¾åˆ°ç§»é™¤åˆ—è¡¨æ–‡ä»¶ '{remove_txt}', å°†å¤åˆ¶æ‰€æœ‰ episodesã€‚")

    # åŠ è½½ meta æ–‡ä»¶
    episodes_path = src_meta / "episodes.jsonl"
    if not episodes_path.exists():
        print(f"    - âŒ é”™è¯¯: æ‰¾ä¸åˆ°å…ƒæ•°æ®æ–‡ä»¶ {episodes_path}ã€‚è·³è¿‡æ­¤æ•°æ®é›†ã€‚")
        return
    episodes = load_jsonl(episodes_path)

    stats_path = src_meta / "episodes_stats.jsonl"
    if not stats_path.exists():
        print(f"    - âŒ é”™è¯¯: æ‰¾ä¸åˆ°å…ƒæ•°æ®æ–‡ä»¶ {stats_path}ã€‚è·³è¿‡æ­¤æ•°æ®é›†ã€‚")
        return
    stats = load_jsonl(stats_path)

    # ä¿ç•™æœªåˆ é™¤çš„ entries
    filtered = [
        (ep, st) for ep, st in zip(episodes, stats)
        if f"{ep['episode_index']:06d}" not in remove_ids
    ]
    
    if not filtered:
        print(f"    - âš ï¸ è­¦å‘Š: è¿‡æ»¤åæ²¡æœ‰å‰©ä½™çš„ episodesã€‚è·³è¿‡æ­¤æ•°æ®é›†ã€‚")
        return

    # æŒ‰é¡ºåºå¤„ç†å‰©ä¸‹çš„ episode
    for new_idx, (ep, st) in enumerate(filtered):
        old_idx_str = f"{ep['episode_index']:06d}"
        new_idx_str = f"{new_idx:06d}"

        # æ›´æ–° JSON ä¸­çš„ episode_index å­—æ®µ
        ep["episode_index"] = new_idx
        st["episode_index"] = new_idx

        # ä¿®æ”¹ parquet ä¸­çš„ episode_index å­—æ®µ
        old_parquet = src_data / f"episode_{old_idx_str}.parquet"
        new_parquet = dst_data / f"episode_{new_idx_str}.parquet"
        if old_parquet.exists():
            df = pd.read_parquet(old_parquet)
            if "episode_index" in df.columns:
                df["episode_index"] = new_idx
            else:
                print(f"    - âš ï¸ è­¦å‘Š: 'episode_index' not found in {old_parquet.name}")
            df.to_parquet(new_parquet)

        # æ‹·è´å¯¹åº”è§†é¢‘æ–‡ä»¶
        for cam in cam_list:
            old_mp4 = src_videos[cam] / f"episode_{old_idx_str}.mp4"
            new_mp4 = dst_videos[cam] / f"episode_{new_idx_str}.mp4"
            if old_mp4.exists():
                shutil.copy2(old_mp4, new_mp4)

    # ä¿å­˜æ›´æ–°åçš„ meta æ–‡ä»¶
    save_jsonl(dst_meta / "episodes.jsonl", [ep for ep, _ in filtered])
    save_jsonl(dst_meta / "episodes_stats.jsonl", [st for _, st in filtered])

    # å¤åˆ¶å…¶ä»–å…ƒæ•°æ®æ–‡ä»¶
    if modality_file_path.exists():
        shutil.copy2(modality_file_path, dst_meta / "modality.json")
    if (src_meta / "tasks.jsonl").exists():
        shutil.copy2(src_meta / "tasks.jsonl", dst_meta / "tasks.jsonl")

    # æ›´æ–° info.json
    info_path_src = src_meta / "info.json"
    info_path_dst = dst_meta / "info.json"
    if info_path_src.exists():
        with open(info_path_src, 'r') as f:
            info = json.load(f)
        info["total_episodes"] = len(filtered)
        info["total_videos"] = len(cam_list) * len(filtered)
        info["splits"]["train"] = f"0:{len(filtered)}"
        with open(info_path_dst, 'w') as f:
            json.dump(info, f, indent=2)

    print(f"    - âœ”ï¸ æ¸…ç†å’Œå¤åˆ¶å®Œæˆï¼å…±ä¿ç•™ {len(filtered)} ä¸ª episodesã€‚")
    print(f"    - â— è¯·å†æ¬¡æ£€æŸ¥ {dst_meta / 'tasks.jsonl'} çš„æ˜ å°„æ˜¯å¦æ­£ç¡®ã€‚")


def main():
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨åŒ–æŸ¥æ‰¾ã€è´¨æ£€å’Œæ¸…ç† LeRobot æ•°æ®é›†çš„æµæ°´çº¿ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "--src_base_path", type=str, required=True,
        help="è¦å¼€å§‹æœç´¢çš„æºæ ¹ç›®å½•è·¯å¾„ã€‚\nä¾‹å¦‚: /pfs/pfs-ahGxdf/data/collect_data/so101"
    )
    parser.add_argument(
        "--dst_base_path", type=str, required=True,
        help="ç”¨äºå­˜æ”¾å·²å¤„ç†æ•°æ®é›†çš„ç›®æ ‡æ ¹ç›®å½•è·¯å¾„ã€‚\nä¾‹å¦‚: /pfs/pfs-ahGxdf/data/xiezhengyuan/backup/generalizable_pick_place_processed"
    )
    parser.add_argument(
        "--search_dirs", type=str, default="*",
        help="åœ¨ src_base_path ä¸‹è¦æœç´¢çš„å­ç›®å½•ï¼Œç”¨é€—å·åˆ†éš”ã€‚\næ”¯æŒé€šé…ç¬¦ '*'ã€‚é»˜è®¤ä¸º '*' (æœç´¢æ‰€æœ‰å­ç›®å½•)ã€‚\nä¾‹å¦‚: blk0,blk3"
    )
    parser.add_argument(
        "--modality_path", type=str, required=True,
        help="é€šç”¨çš„ modality.json æ–‡ä»¶è·¯å¾„ã€‚\nä¾‹å¦‚: /path/to/modality.json"
    )
    parser.add_argument(
        "--cams", type=str, default="front,wrist",
        help="é€—å·åˆ†éš”çš„ç›¸æœºåç§°åˆ—è¡¨ï¼Œé»˜è®¤ä¸º 'front,wrist'ã€‚"
    )
    parser.add_argument(
        "--validator_script", type=str, default=None,
        help="(å¯é€‰) ç”¨äºè§†é¢‘è´¨æ£€çš„ Python è„šæœ¬è·¯å¾„ã€‚\nè¯¥è„šæœ¬åº”æ¥å—ä¸€ä¸ªæ•°æ®é›†è·¯å¾„ä½œä¸ºå‚æ•°ï¼Œå¹¶åœ¨è¯¥è·¯å¾„ä¸‹ç”Ÿæˆ 'low_quality.txt'ã€‚\nä¾‹å¦‚: video_check/validate_videos.py"
    )
    parser.add_argument(
        "--manual_remove", type=json.loads, default={},
        help="ä¸€ä¸ªJSONå­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å®šæ‰‹åŠ¨ç§»é™¤çš„ episode IDã€‚\né”®æ˜¯ç›¸å¯¹äº src_base_path çš„æ•°æ®é›†è·¯å¾„ï¼Œå€¼æ˜¯é€—å·åˆ†éš”çš„IDå­—ç¬¦ä¸²ã€‚\nç¤ºä¾‹: '{\"blk0/20250825_blk0\": \"10,25\", \"blk3/another_data\": \"5\"}'"
    )
    
    args = parser.parse_args()

    # å°†é€—å·åˆ†éš”çš„æœç´¢ç›®å½•è½¬æ¢ä¸ºåˆ—è¡¨
    search_dirs = [d.strip() for d in args.search_dirs.split(',')]

    all_found_datasets = []
    for directory in search_dirs:
        search_path = Path(args.src_base_path) / directory
        if '*' not in directory and '?' not in directory:
             all_found_datasets.extend(find_dataset_folders(search_path))
        else:
            for matching_dir in Path(args.src_base_path).glob(directory):
                if matching_dir.is_dir():
                    all_found_datasets.extend(find_dataset_folders(matching_dir))
    
    if not all_found_datasets:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„æ•°æ®é›†æ–‡ä»¶å¤¹ã€‚è¯·æ£€æŸ¥ --src_base_path å’Œ --search_dirs å‚æ•°ã€‚")
        return
        
    print(f"\nâœ¨ æ€»å…±æ‰¾åˆ° {len(all_found_datasets)} ä¸ªæ•°æ®é›†ï¼Œå³å°†å¼€å§‹å¤„ç†...\n" + "="*80)

    for i, src_path in enumerate(all_found_datasets):
        print(f"\n({i+1}/{len(all_found_datasets)}) æ­£åœ¨å¤„ç†: {src_path}")
        print("-" * 60)

        # ### æ–°å¢é€»è¾‘ ###
        # 1. æ„å»ºç›®æ ‡è·¯å¾„å¹¶æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ã€‚å¦‚æœå­˜åœ¨ï¼Œåˆ™è·³è¿‡ã€‚
        relative_path_str = str(src_path.relative_to(args.src_base_path))
        dst_path = Path(args.dst_base_path) / relative_path_str
        
        if dst_path.is_dir():
            print(f"  ğŸŸ¡ ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {dst_path}")
            continue
        # ### é€»è¾‘ç»“æŸ ###

        # 2. è¿è¡Œè§†é¢‘è´¨æ£€ï¼Œè·å– remove_txt è·¯å¾„
        remove_txt_path = run_video_validation(src_path, args.validator_script)

        # 3. ç»“åˆæ‰‹åŠ¨æŒ‡å®šçš„ç§»é™¤åˆ—è¡¨
        manual_ids_for_this_dataset = args.manual_remove.get(relative_path_str, "")
        combine_manual_removals(remove_txt_path, manual_ids_for_this_dataset)

        # 4. æ‰§è¡Œæ¸…ç†å’Œå¤åˆ¶
        try:
            clean_and_copy_dataset(
                src_root=src_path,
                dst_root=dst_path,
                remove_txt=remove_txt_path,
                cams=args.cams,
                modality_file_path=Path(args.modality_path)
            )
        except Exception as e:
            print(f"    - âŒ å¤„ç†æ•°æ®é›† {src_path} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    print("\n" + "="*80 + f"\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼å…±å¤„ç†äº† {len(all_found_datasets)} ä¸ªæ•°æ®é›†ã€‚")


if __name__ == "__main__":
    main()